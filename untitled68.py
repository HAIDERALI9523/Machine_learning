# -*- coding: utf-8 -*-
"""Untitled68.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A3uNJrNX2sEuRA4S98979-y4IK8jM-ay
"""

import pandas as pd
from collections import Counter
import re
import plotly.express as px
import plotly.io as pio
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest


pio.templates.default = "plotly_white"

# Import the dataset
df = pd.read_csv("/content/Queries.csv")

# Check for null values
null_values = df.isnull().sum()

# Display column information
column_info = df.info()

# Descriptive statistics
statistics = df.describe()

print("Null Values:")
print(null_values)
print("\nColumn Information:")
print(column_info)
print("\nDescriptive Statistics:")
print(statistics)

print("Data type of CTR column before conversion:")
print(df['CTR'].dtype)

df['CTR'] = df['CTR'].apply(lambda x: float(x.strip('%')) / 100)

print("\nData type of CTR column after conversion:")
print(df['CTR'].dtype)

#  clean and split
def clean_and_split(query):

    query = re.sub(r'[^a-zA-Z\s]', '', query)
    query = query.lower()

    return query.split()

# Apply the clean_and_split function
df['Cleaned Queries'] = df['Top queries'].apply(clean_and_split)

word_list = [word for sublist in df['Cleaned Queries'] for word in sublist]

word_counts = Counter(word_list)

word_freq_df = pd.DataFrame(word_counts.items(), columns=['Word', 'Frequency'])

word_freq_df = word_freq_df.sort_values(by='Frequency', ascending=False)

fig = px.bar(word_freq_df.head(20), x='Word', y='Frequency', title='Top 20 Common Words in Search Queries')
fig.show()

top_queries_by_clicks = df.sort_values(by='Clicks', ascending=False).head(10)
top_queries_by_impressions = df.sort_values(by='Impressions', ascending=False).head(10)

# Plot top queries by clicks
fig_clicks = px.bar(top_queries_by_clicks, x='Top queries', y='Clicks', title='Top Queries by Clicks')
fig_clicks.show()

# Plot top queries by impressions
fig_impressions = px.bar(top_queries_by_impressions, x='Top queries', y='Impressions', title='Top Queries by Impressions')
fig_impressions.show()

lowest_ctr_queries = df.sort_values(by='CTR', ascending=True).head(10)

highest_ctr_queries = df.sort_values(by='CTR', ascending=False).head(10)

# Plot queries with lowest CTRs
fig_lowest_ctr = px.bar(lowest_ctr_queries, x='Top queries', y='CTR', title='Queries with Lowest CTRs')
fig_lowest_ctr.show()

# Plot queries with highest CTRs
fig_highest_ctr = px.bar(highest_ctr_queries, x='Top queries', y='CTR', title='Queries with Highest CTRs')
fig_highest_ctr.show()

df['CTR'] = pd.to_numeric(df['CTR'], errors='coerce')

invalid_ctr_rows = df[df['CTR'].isnull()]
print("Rows with non-numeric values in 'CTR' column:")
print(invalid_ctr_rows)

df = df.dropna(subset=['CTR'])

# Compute the correlation matrix
correlation_matrix = df.corr()

# Plot the correlation matrix using a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Matrix')
plt.show()

df['CTR'] = df['CTR'].str.rstrip('%').astype('float') / 100

numeric_columns = ['Clicks', 'Impressions', 'CTR', 'Position']
X = df[numeric_columns]


isolation_forest = IsolationForest(random_state=0)

isolation_forest.fit(X)


predictions = isolation_forest.predict(X)

df['Anomaly'] = predictions

# Display rows with anomalies (outliers)
anomalies = df[df['Anomaly'] == -1]
print("Anomalies (Outliers):")
print(anomalies)